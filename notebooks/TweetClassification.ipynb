{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Awl3jnQg0qRp"
   },
   "source": [
    "# Tweet Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xnr4V8QQUe3y"
   },
   "source": [
    "## Data Set Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KQMAyjUSB1C"
   },
   "source": [
    "### DATA Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dACFTDGB0pqQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1vy_sPgu04ci"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hassa\\AppData\\Local\\Temp\\ipykernel_12844\\2020695786.py:3: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pd.read_csv('D:/DNNforBotDetection/DNNforBotDetection/input/cresci-2017.csv/Tweets/Social_spambots_1.csv', encoding='latin-1'),\n",
      "C:\\Users\\hassa\\AppData\\Local\\Temp\\ipykernel_12844\\2020695786.py:4: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pd.read_csv('D:/DNNforBotDetection/DNNforBotDetection/input/cresci-2017.csv/Tweets/social_spambots_2.csv', encoding='latin-1'),\n",
      "C:\\Users\\hassa\\AppData\\Local\\Temp\\ipykernel_12844\\2020695786.py:5: DtypeWarning: Columns (7,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pd.read_csv('D:/DNNforBotDetection/DNNforBotDetection/input/cresci-2017.csv/Tweets/social_spambots_3.csv', encoding='latin-1')\n",
      "C:\\Users\\hassa\\AppData\\Local\\Temp\\ipykernel_12844\\2020695786.py:9: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  clean_tweets = pd.read_csv('D:/DNNforBotDetection/DNNforBotDetection/input/cresci-2017.csv/Tweets/genuine_accounts.csv', encoding='latin-1')\n"
     ]
    }
   ],
   "source": [
    "# Concatenate social spambot tweets\n",
    "bot_tweets = pd.concat([\n",
    "    pd.read_csv('D:/DNNforBotDetection/DNNforBotDetection/input/cresci-2017.csv/Tweets/Social_spambots_1.csv', encoding='latin-1'),\n",
    "    pd.read_csv('D:/DNNforBotDetection/DNNforBotDetection/input/cresci-2017.csv/Tweets/social_spambots_2.csv', encoding='latin-1'),\n",
    "    pd.read_csv('D:/DNNforBotDetection/DNNforBotDetection/input/cresci-2017.csv/Tweets/social_spambots_3.csv', encoding='latin-1')\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# Read genuine account tweets\n",
    "clean_tweets = pd.read_csv('D:/DNNforBotDetection/DNNforBotDetection/input/cresci-2017.csv/Tweets/genuine_accounts.csv', encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1-tmVXi1CqB",
    "outputId": "f961fb80-e6a3-4040-9c20-785fe4103cc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                           int64\n",
      "text                        object\n",
      "source                      object\n",
      "user_id                      int64\n",
      "truncated                  float64\n",
      "in_reply_to_status_id        int64\n",
      "in_reply_to_user_id          int64\n",
      "in_reply_to_screen_name     object\n",
      "retweeted_status_id          int64\n",
      "geo                        float64\n",
      "place                       object\n",
      "contributors               float64\n",
      "retweet_count                int64\n",
      "reply_count                  int64\n",
      "favorite_count               int64\n",
      "favorited                  float64\n",
      "retweeted                  float64\n",
      "possibly_sensitive         float64\n",
      "num_hashtags                 int64\n",
      "num_urls                     int64\n",
      "num_mentions                 int64\n",
      "created_at                  object\n",
      "timestamp                   object\n",
      "crawled_at                  object\n",
      "updated                     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(bot_tweets.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyGSaCxpVFMr"
   },
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tEnVtfbZVMr-"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hceYdUtG1EpR"
   },
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['truncated', 'in_reply_to_status_id', 'in_reply_to_user_id', 'in_reply_to_screen_name',\n",
    "                   'retweeted_status_id', 'geo', 'place', 'contributors', 'favorited', 'retweeted',\n",
    "                   'possibly_sensitive', 'created_at', 'timestamp', 'crawled_at', 'updated']\n",
    "\n",
    "# Drop the columns from the DataFrame\n",
    "bot_tweets = bot_tweets.drop(columns=columns_to_drop)\n",
    "clean_tweets = clean_tweets.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 50% of samples to make it computationally feasible\n",
    "\n",
    "clean_tweets = clean_tweets.drop(clean_tweets.sample(frac=0.5, random_state=42).index)\n",
    "bot_tweets = bot_tweets.drop(bot_tweets.sample(frac=0.5, random_state=42).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JVKs_V3QI28S",
    "outputId": "a5f08d71-8f5d-4fea-f043-f9294974eb2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 int64\n",
       "text              object\n",
       "source            object\n",
       "user_id            int64\n",
       "retweet_count      int64\n",
       "reply_count        int64\n",
       "favorite_count     int64\n",
       "num_hashtags       int64\n",
       "num_urls           int64\n",
       "num_mentions       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot_tweets.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lG9SbTtWJA8W"
   },
   "outputs": [],
   "source": [
    "bot_tweets['BotOrNot'] = 1\n",
    "clean_tweets['BotOrNot'] = 0\n",
    "\n",
    "Tweets1_df = pd.concat([bot_tweets, clean_tweets])\n",
    "\n",
    "Tweets_df = Tweets1_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "z_EaOl0OXUa0"
   },
   "outputs": [],
   "source": [
    "# Remove null samples from the 'text' column\n",
    "Tweets_df = Tweets_df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LpFSUKq-rjYg"
   },
   "outputs": [],
   "source": [
    "def replace_hashtags(tweet):\n",
    "    return re.sub(r'#\\w+', '<hashtag>', tweet)\n",
    "\n",
    "def replace_urls(tweet):\n",
    "    return re.sub(r'http\\S+|www\\S+', '<url>', tweet)\n",
    "\n",
    "def replace_numbers(tweet):\n",
    "    return re.sub(r'\\d+', '<number>', tweet)\n",
    "\n",
    "def replace_user_mentions(tweet):\n",
    "    return re.sub(r'@\\w+', '<user>', tweet)\n",
    "\n",
    "def replace_emojis(tweet):\n",
    "    emoji_dict = {\n",
    "        \":)\": \"<smile>\",\n",
    "        \"<3\": \"<heart>\",\n",
    "        \":D\": \"<lolface>\",\n",
    "        \":|\": \"<neutralface>\",\n",
    "        \">:(\": \"<angryface>\"\n",
    "    }\n",
    "    for emoticon, tag in emoji_dict.items():\n",
    "        tweet = tweet.replace(emoticon, tag)\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    tweet = re.sub(r':\\w+:', '', tweet)\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Pj8EQqxIVeu2"
   },
   "outputs": [],
   "source": [
    "def add_allcaps_tag(word):\n",
    "    if word.isupper() or re.search(r'(\\w)\\1{2,}', word):\n",
    "        return f\"{word} <allcaps>\"\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xSalJWqGVh9y"
   },
   "outputs": [],
   "source": [
    "def preprocess_tweets(df):\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    df['text'] = df['text'].apply(replace_hashtags)\n",
    "    df['text'] = df['text'].apply(replace_urls)\n",
    "    df['text'] = df['text'].apply(replace_numbers)\n",
    "    df['text'] = df['text'].apply(replace_user_mentions)\n",
    "    df['text'] = df['text'].apply(replace_emojis)\n",
    "    df['text'] = df['text'].apply(lambda tweet: ' '.join(add_allcaps_tag(word) for word in tweet.split()))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "augC43xjVk_7"
   },
   "outputs": [],
   "source": [
    "Final_Tweet_df = preprocess_tweets(Tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7vdNqFT-O4g-",
    "outputId": "cf0ccfcf-2484-469c-83f2-5e6129e2b039",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>user_id</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_urls</th>\n",
       "      <th>num_mentions</th>\n",
       "      <th>BotOrNot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>533098073870704640</td>\n",
       "      <td>rt &lt;user&gt;: these are the kind of pictures that...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>5.495493e+08</td>\n",
       "      <td>16372.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>583206953452748800</td>\n",
       "      <td>holly fuck lauren why are you so cute &lt;url&gt;</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>2.251879e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>514615231088455680</td>\n",
       "      <td>my wife is such a terrible cook the police mad...</td>\n",
       "      <td>&lt;a href=\"http://tweetadder.com\" rel=\"nofollow\"...</td>\n",
       "      <td>5.311497e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>529397298162184192</td>\n",
       "      <td>man overboard &lt;hashtag&gt; by &lt;user&gt; \"the most la...</td>\n",
       "      <td>&lt;a href=\"http://tweetadder.com\" rel=\"nofollow\"...</td>\n",
       "      <td>3.709602e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>528262761097478144</td>\n",
       "      <td>un giorno comprenderã² come possa provare tant...</td>\n",
       "      <td>&lt;a href=\"http://www.emanueladuccio.it\" rel=\"no...</td>\n",
       "      <td>4.663804e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148243</th>\n",
       "      <td>495233689857843200</td>\n",
       "      <td>tutto questo amore a parole, senza potersi toc...</td>\n",
       "      <td>&lt;a href=\"http://www.giorgiasandri1.com\" rel=\"n...</td>\n",
       "      <td>4.664575e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148244</th>\n",
       "      <td>410163920482357248</td>\n",
       "      <td>&lt;user&gt; suertuda!</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/#!/download/ipad\" ...</td>\n",
       "      <td>1.536902e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148245</th>\n",
       "      <td>549118737971154945</td>\n",
       "      <td>\"yesterday was a thousand years ago... in all ...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>4.946875e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148246</th>\n",
       "      <td>569457982724943872</td>\n",
       "      <td>lack of sleep</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>2.286597e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148247</th>\n",
       "      <td>274307161973223425</td>\n",
       "      <td>amazing!!!! &lt;url&gt;</td>\n",
       "      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;...</td>\n",
       "      <td>2.041228e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3141714 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0        533098073870704640   \n",
       "1        583206953452748800   \n",
       "2        514615231088455680   \n",
       "3        529397298162184192   \n",
       "4        528262761097478144   \n",
       "...                     ...   \n",
       "3148243  495233689857843200   \n",
       "3148244  410163920482357248   \n",
       "3148245  549118737971154945   \n",
       "3148246  569457982724943872   \n",
       "3148247  274307161973223425   \n",
       "\n",
       "                                                      text  \\\n",
       "0        rt <user>: these are the kind of pictures that...   \n",
       "1              holly fuck lauren why are you so cute <url>   \n",
       "2        my wife is such a terrible cook the police mad...   \n",
       "3        man overboard <hashtag> by <user> \"the most la...   \n",
       "4        un giorno comprenderã² come possa provare tant...   \n",
       "...                                                    ...   \n",
       "3148243  tutto questo amore a parole, senza potersi toc...   \n",
       "3148244                                   <user> suertuda!   \n",
       "3148245  \"yesterday was a thousand years ago... in all ...   \n",
       "3148246                                      lack of sleep   \n",
       "3148247                                  amazing!!!! <url>   \n",
       "\n",
       "                                                    source       user_id  \\\n",
       "0        <a href=\"http://twitter.com/download/iphone\" r...  5.495493e+08   \n",
       "1        <a href=\"http://twitter.com/download/android\" ...  2.251879e+09   \n",
       "2        <a href=\"http://tweetadder.com\" rel=\"nofollow\"...  5.311497e+08   \n",
       "3        <a href=\"http://tweetadder.com\" rel=\"nofollow\"...  3.709602e+08   \n",
       "4        <a href=\"http://www.emanueladuccio.it\" rel=\"no...  4.663804e+08   \n",
       "...                                                    ...           ...   \n",
       "3148243  <a href=\"http://www.giorgiasandri1.com\" rel=\"n...  4.664575e+08   \n",
       "3148244  <a href=\"http://twitter.com/#!/download/ipad\" ...  1.536902e+08   \n",
       "3148245  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...  4.946875e+08   \n",
       "3148246  <a href=\"http://twitter.com/download/iphone\" r...  2.286597e+09   \n",
       "3148247  <a href=\"http://instagram.com\" rel=\"nofollow\">...  2.041228e+07   \n",
       "\n",
       "         retweet_count  reply_count  favorite_count  num_hashtags  num_urls  \\\n",
       "0              16372.0          0.0             0.0           0.0       0.0   \n",
       "1                  0.0          0.0             0.0           0.0       0.0   \n",
       "2                  0.0          0.0             0.0           0.0       0.0   \n",
       "3                  0.0          0.0             0.0           1.0       1.0   \n",
       "4                  1.0          0.0             0.0           0.0       0.0   \n",
       "...                ...          ...             ...           ...       ...   \n",
       "3148243            0.0          0.0             1.0           0.0       0.0   \n",
       "3148244            0.0          0.0             0.0           0.0       0.0   \n",
       "3148245            0.0          0.0             1.0           0.0       0.0   \n",
       "3148246            0.0          0.0             0.0           0.0       0.0   \n",
       "3148247            0.0          0.0             0.0           0.0       1.0   \n",
       "\n",
       "         num_mentions  BotOrNot  \n",
       "0                 1.0         0  \n",
       "1                 0.0         0  \n",
       "2                 0.0         1  \n",
       "3                 1.0         1  \n",
       "4                 0.0         1  \n",
       "...               ...       ...  \n",
       "3148243           0.0         1  \n",
       "3148244           1.0         0  \n",
       "3148245           0.0         0  \n",
       "3148246           0.0         0  \n",
       "3148247           0.0         0  \n",
       "\n",
       "[3141714 rows x 11 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_Tweet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IICysZRObKKJ"
   },
   "source": [
    "### GLove model for creating vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "rl2hIzy9VrPo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZE60kagXcfRf",
    "outputId": "6dd4fdd9-37bf-4dfb-d179-6032b122e61c"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_path = \"D:/DNNforBotDetection/DNNforBotDetection/input/glove.twitter.27B.200d.txt\"  # Path to the downloaded GloVe file\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format(glove_path, binary=False, no_header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0KK7aMeDcoh5"
   },
   "outputs": [],
   "source": [
    "def get_tweet_embedding(tokens):\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in word_vectors:\n",
    "            embeddings.append(word_vectors[token])\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)  # Average the word embeddings to get the tweet embedding\n",
    "    return np.zeros(word_vectors.vector_size)  # Return a zero vector if no valid word embeddings found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "CUh-aS4ZdxFk"
   },
   "outputs": [],
   "source": [
    "Final_Tweet_df['embedding'] = Final_Tweet_df['text'].apply(get_tweet_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GWqENbxF2Psl"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense, Activation, Concatenate\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Prepare the data for modeling\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m X_text \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFinal_Tweet_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m X_features \u001b[38;5;241m=\u001b[39m Final_Tweet_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretweet_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreply_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfavorite_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_hashtags\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_urls\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_mentions\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      7\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X_text, X_features), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\numpy\\core\\shape_base.py:432\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    429\u001b[0m axis \u001b[38;5;241m=\u001b[39m normalize_axis_index(axis, result_ndim)\n\u001b[0;32m    431\u001b[0m sl \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m),) \u001b[38;5;241m*\u001b[39m axis \u001b[38;5;241m+\u001b[39m (_nx\u001b[38;5;241m.\u001b[39mnewaxis,)\n\u001b[1;32m--> 432\u001b[0m expanded_arrays \u001b[38;5;241m=\u001b[39m [arr[sl] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(expanded_arrays, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout)\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLP\\lib\\site-packages\\numpy\\core\\shape_base.py:432\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    429\u001b[0m axis \u001b[38;5;241m=\u001b[39m normalize_axis_index(axis, result_ndim)\n\u001b[0;32m    431\u001b[0m sl \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m),) \u001b[38;5;241m*\u001b[39m axis \u001b[38;5;241m+\u001b[39m (_nx\u001b[38;5;241m.\u001b[39mnewaxis,)\n\u001b[1;32m--> 432\u001b[0m expanded_arrays \u001b[38;5;241m=\u001b[39m [arr[sl] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(expanded_arrays, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation, Concatenate\n",
    "# Prepare the data for modeling\n",
    "X_text = np.stack(Final_Tweet_df['embedding'].values)\n",
    "X_features = Final_Tweet_df[['retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions']].values\n",
    "X = np.concatenate((X_text, X_features), axis=1)\n",
    "y = Final_Tweet_df['BotOrNot'].values\n",
    "\n",
    "# LSTM Model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(32, input_shape=(X.shape[1],)))\n",
    "lstm_model.add(Dense(128, activation='relu'))\n",
    "lstm_model.add(Dense(64, activation='relu'))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# DNN Model\n",
    "dnn_model = Sequential()\n",
    "dnn_model.add(Dense(128, activation='relu', input_shape=(X.shape[1],)))\n",
    "dnn_model.add(Dense(64, activation='relu'))\n",
    "dnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Concatenate LSTM and features models\n",
    "combined_model = Sequential()\n",
    "combined_model.add(Concatenate([lstm_model, dnn_model]))\n",
    "combined_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the models\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "dnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "combined_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = combined_model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Retrieve the loss and accuracy\n",
    "loss = history.history['loss']\n",
    "accuracy = history.history['accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Print the final loss and accuracy\n",
    "print(\"Final Loss:\", loss[-1])\n",
    "print(\"Final Accuracy:\", accuracy[-1])\n",
    "\n",
    "# Print the final validation loss and accuracy\n",
    "print(\"Final Validation Loss:\", val_loss[-1])\n",
    "print(\"Final Validation Accuracy:\", val_accuracy[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZjX9WacSqAo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9KQMAyjUSB1C",
    "XyGSaCxpVFMr"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
